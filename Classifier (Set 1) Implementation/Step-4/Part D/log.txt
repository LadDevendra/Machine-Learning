LOG FOR ROW 1 in table.. Ratio 80:20

> library(corrplot)
> library(rpart)
> library(e1071)
> library(class)
> library(neuralnet)
> ####################################################################################
> #DATA AND PREPROCESSING
> #Data Set
> given_Data <- read.csv("processed.cleveland.data.txt",header = FALSE)
> #Adding column Names to the dataset
> colnames(given_Data) <- c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
> #Converting dataset variables to a factor in R
> given_Data$num <- factor(given_Data$num)
> #Summary on the class labels
> summary(given_Data$num)
  0   1   2   3   4 
164  55  36  35  13 
> #Dimensions of the dataset
> dim(given_Data)
[1] 303  14
> #Histogram on the classification labels
> plot(given_Data$num)
> #converting the data to numeric due to error in scaling - Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
> given_Data$age<-as.numeric(given_Data$age)
> given_Data$sex<-as.numeric(given_Data$sex)
> given_Data$cp<-as.numeric(given_Data$cp)
> given_Data$trestbps<-as.numeric(given_Data$trestbps)
> given_Data$chol<-as.numeric(given_Data$chol)
> given_Data$fbs<-as.numeric(given_Data$fbs)
> given_Data$restecg<-as.numeric(given_Data$restecg)
> given_Data$thalach<-as.numeric(given_Data$thalach)
> given_Data$exang<-as.numeric(given_Data$exang)
> given_Data$oldpeak<-as.numeric(given_Data$oldpeak)
> given_Data$slope<-as.numeric(given_Data$slope)
> given_Data$ca<-as.numeric(given_Data$ca)
> given_Data$thal<-as.numeric(given_Data$thal)
> given_Data$num<-as.numeric(given_Data$num)
> #The dataset attributes have to be numeric for correlation plots
> ModelCor <- cor(given_Data)
> corrplot(ModelCor, method="circle")
> corrplot(ModelCor, method="number")
> #scaling the data
> maxs =	apply(given_Data,	MARGIN	=	2,	max)
> mins =	apply(given_Data,	MARGIN	=	2,	min)
> scaled =	as.data.frame(scale(given_Data,	scale	=	maxs	- mins))
> ####################################################################################
> #Decision Tree
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ #summary(testDataSet$num)
+ #Building the model using training data
+ treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
+ classval<- predict(treeModel,testDataSet,type=c("class"))
+ table(classval,testDataSet$num)
+ #Accuracy
+ accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
+ print(accuracy)
+ }
[1] 0.4754098
[1] 0.557377
[1] 0.5409836
[1] 0.557377
[1] 0.5737705
> ####################################################################################
> #Perceptron
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ #Building the model using training data
+ nn	<- neuralnet(f,data=trainingDataSet,hidden = 0,r =550, stepmax = 1e+8)
+ #Predicting using test data
+ nn$result.matrix
+ pred	<- compute(nn,testDataSet[,1:13])
+ pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
+ #Accuracy
+ accuracy<- 100- MSE.nn
+ print(accuracy)
+ }
[1] 99.37462105
[1] 99.38929987
[1] 99.03782501
[1] 99.07030267
[1] 99.25977578
> ####################################################################################
> #NeuralNet
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ #Building the model using training data
+ nn	<- neuralnet(f,data=trainingDataSet, hidden=3, rep=2)
+ #Predicting using test data
+ nn$result.matrix
+ pred	<- compute(nn,testDataSet[,1:13])
+ pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
+ #Accuracy
+ accuracy<- 100- MSE.nn
+ print(accuracy)
+ }
[1] 98.5360243
[1] 96.84384151
[1] 98.60208949
[1] 98.80697918
[1] 98.71237444
> ####################################################################################
> #SVM
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
+ trainingDataSet<-given_Data[instances,]
+ testDataSet<-given_Data[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ svmmodel<-svm(f,data=trainingDataSet,cost=100, gamma=0.00005, kernel = 'linear', type = "C-classification")
+ classVal<-predict(svmmodel,testDataSet,type="class")
+ #Accuracy using Confusion matrix
+ pred <- table(classVal,testDataSet$num)
+ table(classVal,testDataSet$num)
+ accuracy<-sum(diag(pred))/sum(pred)
+ print(accuracy)
+ }
[1] 0.5737704918
[1] 0.6557377049
[1] 0.6393442623
[1] 0.5573770492
[1] 0.4918032787
> ####################################################################################
> #Naive base
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained. 
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
+ 
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ 
+ nav_bayes <- naiveBayes(as.factor(num)~., data = trainingDataSet, threshold = 0.001, laplace = 0);
+ naivebayes.pred <- predict(nav_bayes, testDataSet[,1:13]);
+ pred <- table(naivebayes.pred, testDataSet[,14]);
+ 
+ #print(pred)
+ #accuracy
+ accuracy<-sum(diag(pred))/sum(pred)
+ print(accuracy)
+ }
[1] 0.3606557377
[1] 0.4262295082
[1] 0.5573770492
[1] 0.4098360656
[1] 0.4918032787
> ###################################################################################

LOG FOR ROW 1 in table.. Ratio 85:15

> library(corrplot)
> library(rpart)
> library(e1071)
> library(class)
> library(neuralnet)
> ####################################################################################
> #DATA AND PREPROCESSING
> #Data Set
> given_Data <- read.csv("processed.cleveland.data.txt",header = FALSE)
> #Adding column Names to the dataset
> colnames(given_Data) <- c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
> #Converting dataset variables to a factor in R
> given_Data$num <- factor(given_Data$num)
> #Summary on the class labels
> summary(given_Data$num)
  0   1   2   3   4 
164  55  36  35  13 
> #Dimensions of the dataset
> dim(given_Data)
[1] 303  14
> #Histogram on the classification labels
> plot(given_Data$num)
> #converting the data to numeric due to error in scaling - Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
> given_Data$age<-as.numeric(given_Data$age)
> given_Data$sex<-as.numeric(given_Data$sex)
> given_Data$cp<-as.numeric(given_Data$cp)
> given_Data$trestbps<-as.numeric(given_Data$trestbps)
> given_Data$chol<-as.numeric(given_Data$chol)
> given_Data$fbs<-as.numeric(given_Data$fbs)
> given_Data$restecg<-as.numeric(given_Data$restecg)
> given_Data$thalach<-as.numeric(given_Data$thalach)
> given_Data$exang<-as.numeric(given_Data$exang)
> given_Data$oldpeak<-as.numeric(given_Data$oldpeak)
> given_Data$slope<-as.numeric(given_Data$slope)
> given_Data$ca<-as.numeric(given_Data$ca)
> given_Data$thal<-as.numeric(given_Data$thal)
> given_Data$num<-as.numeric(given_Data$num)
> #The dataset attributes have to be numeric for correlation plots
> ModelCor <- cor(given_Data)
> corrplot(ModelCor, method="circle")
> corrplot(ModelCor, method="number")
> #scaling the data
> maxs =	apply(given_Data,	MARGIN	=	2,	max)
> mins =	apply(given_Data,	MARGIN	=	2,	min)
> scaled =	as.data.frame(scale(given_Data,	scale	=	maxs	- mins))
> ####################################################################################
> #Decision Tree
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.85*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ #summary(testDataSet$num)
+ #Building the model using training data
+ treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
+ classval<- predict(treeModel,testDataSet,type=c("class"))
+ table(classval,testDataSet$num)
+ #Accuracy
+ accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
+ print(accuracy)
+ }
[1] 0.652173913
[1] 0.5652173913
[1] 0.6086956522
[1] 0.4782608696
[1] 0.5217391304
> ####################################################################################
> #Perceptron
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.85*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ #Building the model using training data
+ nn	<- neuralnet(f,data=trainingDataSet,hidden = 0,r =550, stepmax = 1e+8)
+ #Predicting using test data
+ nn$result.matrix
+ pred	<- compute(nn,testDataSet[,1:13])
+ pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
+ #Accuracy
+ accuracy<- 100- MSE.nn
+ print(accuracy)
+ }
[1] 99.19669561
[1] 99.25287039
[1] 99.33250649
[1] 99.24399067
[1] 99.2965411
> ####################################################################################
> #NeuralNet
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.85*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ #Building the model using training data
+ nn	<- neuralnet(f,data=trainingDataSet, hidden=3, rep=2)
+ #Predicting using test data
+ nn$result.matrix
+ pred	<- compute(nn,testDataSet[,1:13])
+ pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
+ #Accuracy
+ accuracy<- 100- MSE.nn
+ print(accuracy)
+ }
[1] 98.90783165
[1] 98.64181965
[1] 98.97579428
[1] 99.00321931
[1] 99.45655723
> ####################################################################################
> #SVM
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.85*nrow(scaled))
+ trainingDataSet<-given_Data[instances,]
+ testDataSet<-given_Data[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ svmmodel<-svm(f,data=trainingDataSet,cost=100, gamma=0.00005, kernel = 'linear', type = "C-classification")
+ classVal<-predict(svmmodel,testDataSet,type="class")
+ #Accuracy using Confusion matrix
+ pred <- table(classVal,testDataSet$num)
+ table(classVal,testDataSet$num)
+ accuracy<-sum(diag(pred))/sum(pred)
+ print(accuracy)
+ }
[1] 0.6086956522
[1] 0.5652173913
[1] 0.6739130435
[1] 0.652173913
[1] 0.6739130435
> ####################################################################################
> #Naive base
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.85*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ nav_bayes <- naiveBayes(as.factor(num)~., data = trainingDataSet, threshold = 0.001, laplace = 0);
+ naivebayes.pred <- predict(nav_bayes, testDataSet[,1:13]);
+ pred <- table(naivebayes.pred, testDataSet[,14]);
+ #print(pred)
+ #accuracy
+ accuracy<-sum(diag(pred))/sum(pred)
+ print(accuracy)
+ }
[1] 0.5217391304
[1] 0.5
[1] 0.5434782609
[1] 0.6304347826
[1] 0.5434782609
> ####################################################################################
> 

LOG FOR ROW 1 in table.. Ratio 95:05

> library(corrplot)
> library(rpart)
> library(e1071)
> library(class)
> library(neuralnet)
> ####################################################################################
> #DATA AND PREPROCESSING
> #Data Set
> given_Data <- read.csv("processed.cleveland.data.txt",header = FALSE)
> #Adding column Names to the dataset
> colnames(given_Data) <- c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
> #Converting dataset variables to a factor in R
> given_Data$num <- factor(given_Data$num)
> #Summary on the class labels
> summary(given_Data$num)
  0   1   2   3   4 
164  55  36  35  13 
> #Dimensions of the dataset
> dim(given_Data)
[1] 303  14
> #Histogram on the classification labels
> plot(given_Data$num)
> #converting the data to numeric due to error in scaling - Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
> given_Data$age<-as.numeric(given_Data$age)
> given_Data$sex<-as.numeric(given_Data$sex)
> given_Data$cp<-as.numeric(given_Data$cp)
> given_Data$trestbps<-as.numeric(given_Data$trestbps)
> given_Data$chol<-as.numeric(given_Data$chol)
> given_Data$fbs<-as.numeric(given_Data$fbs)
> given_Data$restecg<-as.numeric(given_Data$restecg)
> given_Data$thalach<-as.numeric(given_Data$thalach)
> given_Data$exang<-as.numeric(given_Data$exang)
> given_Data$oldpeak<-as.numeric(given_Data$oldpeak)
> given_Data$slope<-as.numeric(given_Data$slope)
> given_Data$ca<-as.numeric(given_Data$ca)
> given_Data$thal<-as.numeric(given_Data$thal)
> given_Data$num<-as.numeric(given_Data$num)
> #The dataset attributes have to be numeric for correlation plots
> ModelCor <- cor(given_Data)
> corrplot(ModelCor, method="circle")
> corrplot(ModelCor, method="number")
> #scaling the data
> maxs =	apply(given_Data,	MARGIN	=	2,	max)
> mins =	apply(given_Data,	MARGIN	=	2,	min)
> scaled =	as.data.frame(scale(given_Data,	scale	=	maxs	- mins))
> ####################################################################################
> #Decision Tree
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ #summary(testDataSet$num)
+ #Building the model using training data
+ treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
+ classval<- predict(treeModel,testDataSet,type=c("class"))
+ table(classval,testDataSet$num)
+ #Accuracy
+ accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
+ print(accuracy)
+ }
[1] 0.6875
[1] 0.5
[1] 0.5625
[1] 0.6875
[1] 0.6875
> ####################################################################################
> #Perceptron
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ #Building the model using training data
+ nn	<- neuralnet(f,data=trainingDataSet,hidden = 0,r =550, stepmax = 1e+8)
+ #Predicting using test data
+ nn$result.matrix
+ pred	<- compute(nn,testDataSet[,1:13])
+ pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
+ #Accuracy
+ accuracy<- 100- MSE.nn
+ print(accuracy)
+ }
[1] 99.12526664
[1] 98.39215069
[1] 99.2946344
[1] 99.16089848
[1] 99.40245344
> ####################################################################################
> #NeuralNet
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ #Building the model using training data
+ nn	<- neuralnet(f,data=trainingDataSet, hidden=3, rep=2)
+ #Predicting using test data
+ nn$result.matrix
+ pred	<- compute(nn,testDataSet[,1:13])
+ pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
+ MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
+ #Accuracy
+ accuracy<- 100- MSE.nn
+ print(accuracy)
+ }
[1] 99.17936546
[1] 99.28032569
[1] 98.47259703
[1] 98.60733964
[1] 99.30567087
> ####################################################################################
> #SVM
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
+ trainingDataSet<-given_Data[instances,]
+ testDataSet<-given_Data[-instances,]
+ n <- names(trainingDataSet)
+ f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
+ svmmodel<-svm(f,data=trainingDataSet,cost=100, gamma=0.00005, kernel = 'linear', type = "C-classification")
+ classVal<-predict(svmmodel,testDataSet,type="class")
+ #Accuracy using Confusion matrix
+ pred <- table(classVal,testDataSet$num)
+ table(classVal,testDataSet$num)
+ accuracy<-sum(diag(pred))/sum(pred)
+ print(accuracy)
+ }
[1] 0.5625
[1] 0.5625
[1] 0.625
[1] 0.625
[1] 0.6875
> ####################################################################################
> #Naive base
> accuracy=0
> #We are choosing the BEST of the 5 accuracies obtained.
> #Since, it was difficult to draw conclusion by parameter tuning using the AVG.
> for(i in 1:5)
+ {
+ #Splitting the data
+ instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
+ trainingDataSet<-scaled[instances,]
+ testDataSet<-scaled[-instances,]
+ nav_bayes <- naiveBayes(as.factor(num)~., data = trainingDataSet, threshold = 0.001, laplace = 0);
+ naivebayes.pred <- predict(nav_bayes, testDataSet[,1:13]);
+ pred <- table(naivebayes.pred, testDataSet[,14]);
+ #print(pred)
+ #accuracy
+ accuracy<-sum(diag(pred))/sum(pred)
+ print(accuracy)
+ }
[1] 0.4375
[1] 0.5
[1] 0.625
[1] 0.625
[1] 0.5625
> ####################################################################################

