Here is my summary:

	This project had the students attempting to find the best machine learning classification model for predicting the survivor label for passengers on the Titanic, where the survivor label is a binary label with 1 representing survival and 0 representing death. Several different classifiers were tried and compared, among which were: Random Forest, Decision Tree, Neural Network, Logistic Regression, K-Nearest Neighbors, Bagging, and Ada Boost. To make sure the parameters each classifier uses were as optimal as possible, exhaustive grid search was performed for each classification model and the intermediate and final results for this procedure was returned, which included model the accuracy for the best combination of parameter values. The best performing classifiers from the grid search results were Bagging, Logistic Regression, Ada Boost, Random Forest, and Decision Tree. 
	To also ensure that the best set of features was used for each classification model, the powerset of features was generated and for each set in the powerset, the model was fit on the training set, with the intermediate accuracy recorded. The set of features that led to the best performance was saved, along with the accuracy achieved by using this set of features. Comparing the most successful sets of features, with the initial correlation chart done in preprocessing, gives the students some intuitive sense of what features mattered most in terms of survival. Ticket Fare, Sex, Passenger Ticket Class, and Age all seem to play a big part, no matter which classifier was used. As explained, this makes intuitive sense if you know anything about the history of the Titanic disaster. Women and children were put into life rafts first, and some have speculated that lower class passengers were delayed in escaping the sinking ship. For feature power set, the classifiers that performed best were Ada Boost, Random Forest, Decision Tree, and Bagging. 
	Cross-validation was also performed on each classifier. The models that achieved the best results from cross validation were Random Forest, Decision Tree, Logistic Regression, K-Nearest Neighbors, and Bagging. As a final test, a simple Gaussian Naive Bayes classifier was run on the dataset to see how well it performed. It did not work well with Grid Search or Feature Power Set, since it does not accept parameters in the Sklearn implementations that were used in this project. It achieved decent, but not great, accuracy at around 79%. 
	This project gave the students experience using multiple machine learning classification models. Not only did the students get experience with utilizing machine learning library and choosing models, but also tuning model parameters and choosing the best set of features. It also helped the students get a clear understanding of how machine learning can be applied to real world situations in order to solve a problem and gain insight. The topic of the project was also interesting, and would not normally seem like a scenario where machine learning could be applied. In a way, this made the project all the more interesting. 