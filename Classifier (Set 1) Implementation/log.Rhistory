instances<-sample(1:nrow(scaled),size = 0.9*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
instances<-sample(1:nrow(scaled),size = 0.9*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
instances<-sample(1:nrow(scaled),size = 0.9*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
for(i in 1:5)
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
for(i in 1:5)
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
for(i in 1:5)
{
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
}
for(i in 1:5)
{
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
acclist[[i]] <- accuracy
#print(accuracy)
}
for (i in 1:length(acclist))
{
print(acclist[[i]])
}
for(i in 1:5)
{
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
acclist[[i]] <- accuracy
#print(accuracy)
}
for (i in 1:length(acclist[]))
{
print(acclist[[i]])
}
for(i in 1:5)
{
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
acclist[[i]] <- accuracy
print(acclist[[i]])
#print(accuracy)
}
for (i in 1:length(acclist[]))
{
print(acclist[[i]])
}
for(i in 1:5)
{
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
acclist[[i]] <- accuracy
print(accuracy)
}
for(i in 1:5)
{
# create training data by sampling 80% of the dataset
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 60, cp = 0.009))
val<- predict(treeModel,testDataSet,type=c("class"))
#Accuracy
accuracy <- sum(testDataSet$num == val)/nrow(testDataSet)
print(accuracy)
}
for(i in 1:5)
{
#NeuralNet
accuracy=0
#accuracyArray <- c()
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-given_Data[instances,]
testDataSet<-given_Data[-instances,]
n <- names(trainingDataSet)
f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
#Building the model using training data
nn	<- neuralnet(f,data=trainingDataSet, hidden=c(2,3))
#Predicting using test data
plot(nn)
nn$result.matrix
pred	<- compute(nn,testDataSet[,1:13])
pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
plot(real.values, pred.scaled, col='red',main='Real	vs	predicted	NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red',	bty='n')
#Accuracy
accuracy<- 100- MSE.nn
#accuracyArray <- rbind(accuracyArray,c(i,accuracy*100))
print(accuracy)
}
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.09))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Perceptron
accuracy=0
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
n <- names(trainingDataSet)
f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
#Building the model using training data
nn	<- neuralnet(f,data=trainingDataSet,hidden = 0,r =550, stepmax = 1e+8)
#Predicting using test data
nn$result.matrix
pred	<- compute(nn,testDataSet[,1:13])
pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
#Accuracy
accuracy<- 100- MSE.nn
print(accuracy)
}
#NeuralNet
accuracy=0
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
n <- names(trainingDataSet)
f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
#Building the model using training data
nn	<- neuralnet(f,data=trainingDataSet, hidden=3, rep=2)
#Predicting using test data
nn$result.matrix
pred	<- compute(nn,testDataSet[,1:13])
pred.scaled	<- pred$net.result *(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
real.values	<- (testDataSet$num)*(max(given_Data$num)-min(given_Data$num))+min(given_Data$num)
MSE.nn	<- sum((real.values	- pred.scaled)^2)/nrow(testDataSet)
#Accuracy
accuracy<- 100- MSE.nn
print(accuracy)
}
#SVM
accuracy=0
#We are choosing the AVG of the 5 accuracy obtained, calculated manually
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.8*nrow(scaled))
trainingDataSet<-given_Data[instances,]
testDataSet<-given_Data[-instances,]
n <- names(trainingDataSet)
f	<- as.formula(paste("num	~",	paste(n[!n	%in%	"num"],	collapse	=	"	+	")))
svmmodel<-svm(f,data=trainingDataSet,cost=700000, gamma=0.0001, kernel = 'linear', type = "C-classification")
classVal<-predict(svmmodel,testDataSet,type="class")
#Confusion matrix
pred <- table(classVal,testDataSet$num)
table(classVal,testDataSet$num)
accuracy<-sum(diag(pred))/sum(pred)
print(accuracy)
}
#naive base
accuracy=0
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
nav_bayes <- naiveBayes(as.factor(num)~., data = trainingDataSet, threshold = 0.01, laplace = 0);
naivebayes.pred <- predict(nav_bayes, testDataSet[,1:13]);
pred <- table(naivebayes.pred, testDataSet[,14]);
print(pred)
accuracy<-sum(diag(pred))/sum(pred)
print(accuracy)
}
#naive base
accuracy=0
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.95*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
nav_bayes <- naiveBayes(as.factor(num)~., data = trainingDataSet, threshold = 0.01, laplace = 0);
naivebayes.pred <- predict(nav_bayes, testDataSet[,1:13]);
pred <- table(naivebayes.pred, testDataSet[,14]);
#print(pred)
accuracy<-sum(diag(pred))/sum(pred)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Decision Tree
#We are choosing the BEST of the 5 accuracies obtained.
#Since, it was difficult to draw conclusion by parameter tuning using the AVG.
for(i in 1:5)
{
#Splitting the data
instances<-sample(1:nrow(scaled),size = 0.80*nrow(scaled))
trainingDataSet<-scaled[instances,]
testDataSet<-scaled[-instances,]
#summary(testDataSet$num)
#Building the model using training data
treeModel<-rpart(as.factor(num)~.,method='class',data=trainingDataSet,control = rpart.control(minsplit = 65, cp = 0.009))
classval<- predict(treeModel,testDataSet,type=c("class"))
table(classval,testDataSet$num)
#Accuracy
accuracy <- sum(testDataSet$num == classval)/nrow(testDataSet)
print(accuracy)
}
#Data Set
given_Data <- read.csv("processed.cleveland.data.txt",header = FALSE)
#Adding column Names to the dataset
colnames(given_Data) <- c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")
#Converting dataset variables to a factor in R
given_Data$num <- factor(given_Data$num)
summary(given_Data$num)
dim(given_Data)
plot(given_Data$num)
ModelCor <- cor(t)
corrplot(ModelCor, method="circle")
install.packages("corrplot", dependencies = TRUE)
library(corrplot)
ModelCor <- cor(t)
corrplot(ModelCor, method="circle")
ModelCor <- cor(given_Data)
corrplot(ModelCor, method="circle")
ModelCor <- cor(given_Data)
corrplot(ModelCor, method="circle")
given_Data$age<-as.numeric(given_Data$age)
given_Data$sex<-as.numeric(given_Data$sex)
given_Data$cp<-as.numeric(given_Data$cp)
given_Data$trestbps<-as.numeric(given_Data$trestbps)
given_Data$chol<-as.numeric(given_Data$chol)
given_Data$fbs<-as.numeric(given_Data$fbs)
given_Data$restecg<-as.numeric(given_Data$restecg)
given_Data$thalach<-as.numeric(given_Data$thalach)
given_Data$exang<-as.numeric(given_Data$exang)
given_Data$oldpeak<-as.numeric(given_Data$oldpeak)
given_Data$slope<-as.numeric(given_Data$slope)
given_Data$ca<-as.numeric(given_Data$ca)
given_Data$thal<-as.numeric(given_Data$thal)
given_Data$num<-as.numeric(given_Data$num)
ModelCor <- cor(given_Data)
corrplot(ModelCor, method="circle")
corrplot(ModelCor, method="number")
savehistory("~/log.Rhistory")
